---
layout: post
comments: true
publish: false
title: You're Deploying it Wrong! - AS Edition (Part 4)
date: 2019-10-16
author: Daniel Otykier
authorurl: http://twitter.com/dotykier
---

This is part 4 of the Analysis Services DevOps blog series. [Go to part 3](https://tabulareditor.github.io/2019/10/08/DevOps3.html)

## Refining the build pipeline

In the last chapter, we saw how to set up the first basic build pipeline, which used Tabular Editor to deploy a model from a .bim file or folder structure unto an instance of Analysis Services. It's time to take this one step further. In real life, there are a couple of things we'd like our build pipeline todo. Essentially, we want to make sure that we're producing an artifact that is ready for deployment, so for a Tabular model, this typically means the following:

- **Best Practice Analysis:** Ensure that Best Practice Rules are obeyed.
- **Schema check:** Ensure that the model can connect to its data source(s) and source columns map correctly to imported columns.
- **Validation deployment:** Ensure that the model does not contain invalid DAX or other semantic errors (for example, circular dependencies).
- **Refresh Check:** Ensure that partitions can be refreshed without errors or warnings.
- **Unit Testing:** Ensure that calculations provide expected results.

Unit testing on a Tabular model typically requires some data. For example, it would make sense to check whether the measure `[Sales Amount]` returns the same figure as the SQL query `SELECT SUM([Sales Amount]) FROM fact.ResellerSales`. For this to work, however, we need to have data within the model as well, which would require some kind of refresh. At the same time, we don't want our build pipeline to take too long to complete, which would be the case if Analysis Services had to refresh lots of data. For this reason, consider providing a reduced test dataset for unit testing. There are various ways to do this (which I will cover later), but regardless of how you make the test data available, make sure that the data volume is small enough that a Full refresh can execute in a timely manner (for example, less than 5 minutes).

Even if you don't want to do any unit testing or can't provide a test dataset, I still recommend executing a Calculate refresh within the build pipeline, to make sure that the model doesn't have any bad DAX on calculated tables, calculated columns or calculation groups.

The 5 steps above roughly correspond to the tasks that we need in our build pipeline. Once we have this pipeline set up, we can use [branch policies](https://docs.microsoft.com/en-us/azure/devops/repos/git/branch-policies?view=azure-devops#build-validation), to ensure that all changes build succesfully before a pull request can be approved.

In the following, we will assume that your Tabular Model is saved as a folder structure within the "AdventureWorks" folder in the root of your git repo. If it's located somewhere else, change the first argument of the calls to TabularEditor.exe from `$(Build.SourcesDirectory)\AdventureWorks` to point to the Model.bim file or to the folder housing the database.json file of your tabular model.

### Step 1 - Best Practice Analysis
If you're using Tabular Editors [Best Practice Analyzer](https://github.com/otykier/TabularEditor/wiki/Best-Practice-Analyzer-Improvements) (as you should), it makes a lot of sense to check your model for any rule violations before we do anything else. Most people use the Best Practice Analyzer to check things like naming conventions, that numeric or foreign key columns have been hidden, that Descriptions have been provided for all visible objects, etc. The Tabular Model developer is generally responsible for making sure that no rule violations exist before they commit their code, but just in case someone forgot this, we should run the analysis as part of our build pipeline.

Start by creating a new empty build pipeline and make sure that TabularEditor.exe is available, as described [in the last chapter](https://tabulareditor.github.io/2019/10/08/DevOps3.html#your-first-analysis-services-build-pipeline).

Add a new command line task, with the following command:

```shell
TabularEditor.exe "$(Build.SourcesDirectory)\AdventureWorks" -A -V
```

The `-A` switch instructs Tabular Editor to run the Best Practice Analyzer. This is only meaningful if your model contains a set of rules or links to rule definitions, or if a BPARules.json file exists within the agents `%ProgramData%\TabularEditor` or `%LocalAppData%\TabularEditor` folders.

![image](https://user-images.githubusercontent.com/8976200/67010686-6cd08b80-f0ee-11e9-98e4-8dcaa109087b.png)

If you want to have more control over which rules your model is validated against, you can also specify the path of a rules file like so (assuming there's a BPARules.json file in the root of your git repo):

```shell
TabularEditor.exe "$(Build.SourcesDirectory)\AdventureWorks" -A "$(Build.SourcesDirectory)\BPARules.json" -V
```

In this case, the model will still be validated against rules defined within the model, or rules linked within the model, as on the screenshot above, but rules residing in the `%ProgramData%\TabularEditor` and `%LocalAppData%\TabularEditor` folders are no longer considered.

The -V switch instructs Azure DevOps to treat rule violations differently, depending on the severity level of the rule being broken. Severity level 3 violations are reported as errors (causing the build to fail), level 2 are reported as warnings (causing the build to partially succeed), and level 1 are only informational.

![image](https://user-images.githubusercontent.com/8976200/67011332-a9e94d80-f0ef-11e9-9799-44ffcddd3a01.png)
*Column references should ALWAYS be qualified with the table name. Marco Russo would probably argue that this rule should be set to severity level 3, to cause an error instead of a warning.*

### Step 1 - Validation deployment

Since Tabular Editor can not perform DAX syntax checking and semantic validation, we have to deploy the model to an instance of Analysis Services to make sure there are no such errors. This is exactly what we did [in the last chapter](https://tabulareditor.github.io/2019/10/08/DevOps3.html#your-first-analysis-services-build-pipeline). Let's improve the command line task a little bit, to allow changing data source connection strings and deployment credentials in a more secure manner. As mentioned in the last chapter, a better practice for passing credentials to Tabular Editor, is through environment variables. Simply add an environment variable in the Command Line task editor, pass in the corresponding pipeline variable, and modify the script to use `%EnvironmentVariableName%`:

![image](https://user-images.githubusercontent.com/8976200/66992309-e1dd9a00-f0c9-11e9-9b00-64978922eff2.png)

In the screenshot above, we're transferring the connection string of our Analysis Services instance to Tabular Editor through an environment variable, `%ASConnectionString%`, where as the name of the database to deploy is passed directly from the pipeline variable, `$(ASDatabase)`. Notice the different syntax between the two.

This approach is better than what we did in the previous chapter, because we can still see the executing command in the output of the build, but the credentials will not be exposed anywhere:

![image](https://user-images.githubusercontent.com/8976200/66915235-d1211b80-f018-11e9-8f09-b2907138efdf.png)

#### Updating data sources

**Note**: The technique described below only works for Provider (legacy) data sources.

Model.bim or Database.json files normally don't store credentials for accessing the data sources used by the model. For this reason, we often have to apply these credentials after deploying. Even if we are able to use integrated security, we might still want to change the connection string of a data source, for example in order to point it to a different database. This makes sense in a multi-tiered, multi-environment BI set up: Your production models use data from your production DWH, your UAT models use data from your UAT DWH, etc.

Again, we want to make sure that we can update properties on the data source in a secure way, without exposing sensitive information in build logs, etc.

The solution is to create a Tabular Editor (C#) script, which will be executed from the command line using the `-S` switch immediately before the model is deployed. First, create the following script, and put it within a "scripts" folder in your repository - name it "SetConnectionStringFromEnv.cs":

```csharp
foreach(var dataSource in Model.DataSources.OfType<ProviderDataSource>())
{
    var evName = dataSource.Name.Replace(" ", "") + "ConnectionString";
    var evValue = Environment.GetEnvironmentVariable(evName);
    if (evValue != null)
        dataSource.ConnectionString = evValue;
}
```

This will loop through all (provider) data sources of your model, and replace the connection string on each with the value of an environment variable whose name corresponds to the name of the datasource (with spaces removed) and the "ConnectionString" suffix. So if you have a data source in your model named "SQL DW", the script will use the value of the `SQLDWConnectionString` environment variable as the connection string of the model (provided the environment variable exists).

Now, to execute the script prior to deployment, modify the command line task as follows:

```shell
TabularEditor.exe "$(Build.SourcesDirectory)\AdventureWorks" -S "$(Build.SourcesDirectory)\AdventureWorks\scripts\SetConnectionStringFromEnv.cs" -D "%ASConnectionString%" "$(ASDatabase)" -O -C -P -R -M -W -E -V
```

Also, make sure to assign whatever environment variables and pipeline variables you need:

![image](https://user-images.githubusercontent.com/8976200/66919237-409b0900-f021-11e9-9fd8-c2ce1d96be65.png)

If all goes well, Tabular Editor should output that the script was executed...

![image](https://user-images.githubusercontent.com/8976200/66918500-c9b14080-f01f-11e9-9f53-302d78c304a2.png)

...and the connection string should have been updated on the model that was deployed:

![image](https://user-images.githubusercontent.com/8976200/66918682-2c0a4100-f020-11e9-800f-cb693fb5e36b.png)

### Step 2 - Schema Check

**Note:** Schema Check only works for supported provider (legacy) data sources: OLE DB, ODBC, etc.

Now that we know how to update the connection string on our Tabular Model data sources during build, let's use the updated connection string to perform a schema check. We can use the `-SC` command line switch for this. When we use this switch, Tabular Editor will establish a connection to each data source, and compare the schema of each partition query against the imported columns of the table. If any columns are missing in the source, an error is generated. Other differences (mismatched data types or superfluous columns) will generate a warning.

While we could theoretically put the schema check into Task 1, I prefer having this as a separate step. Start by cloning the previous step, and modify the command line task as follows:

```shell
TabularEditor.exe "$(Build.SourcesDirectory)\AdventureWorks" -S "$(Build.SourcesDirectory)\AdventureWorks\scripts\SetConnectionStringFromEnv.cs" -SC -W -E -V
```

Note how this operation can be performed without deploying to Analysis Services. If you have multiple data sources, make sure to pass in each of them as environment variables:

![image](https://user-images.githubusercontent.com/8976200/66992188-afcc3800-f0c9-11e9-92bd-0605b22c7274.png)

Remember that for this to work, the agent running the build pipeline must be able to reach the data source. This means that if your source is an Azure SQL Databases, for example, the IP of the build agent needs to be whitelisted. For a Microsoft-hosted build agent, you can set the "Allow Azure services and resources to access this server": 

![image](https://user-images.githubusercontent.com/8976200/66920600-e64f7780-f023-11e9-8567-ff701d3135e9.png)

And behold - Tabular Editor now informs us that we have unmapped columns (in this case, it's only a warning, since processing should still succeed, but in the words of Brent Ozar, you're needlessly yelling data across the network, so you might want to look into this at some point):

![image](https://user-images.githubusercontent.com/8976200/66920835-61189280-f024-11e9-80e7-4b0e5f7cb6f3.png)

### Step 3 - Best Practice Analysis
