---
layout: post
comments: true
publish: false
title: You're Deploying it Wrong! - AS Edition (Part 4)
date: 2019-10-16
author: Daniel Otykier
authorurl: http://twitter.com/dotykier
---

This is part 4 of the Analysis Services DevOps blog series. [Go to part 3](https://tabulareditor.github.io/2019/10/08/DevOps3.html)

## Refining the build pipeline

In the last chapter, we saw how to set up the first basic build pipeline, which used Tabular Editor to deploy a model from a .bim file or folder structure unto an instance of Analysis Services. It's time to take this one step further. In real life, there are a couple of things we'd like our build pipeline todo. Essentially, we want to make sure that we're producing an artifact that is ready for deployment, so for a Tabular model, this typically means the following:

- The model does not contain invalid DAX or other semantic errors (for example, circular dependencies)
- The model can connect to its data source(s) and imported columns map correctly to data source columns
- Best Practice Rules are obeyed
- Processing runs without errors or warnings
- Unit tests execute succesfully

Unit testing on a Tabular model typically requires some data. For example, it would make sense to check whether the measure `[Sales Amount]` returns the same figure as the SQL query `SELECT SUM([Sales Amount]) FROM fact.ResellerSales`. For this to work, however, we would need to process data into the model, which does not necessarily happen when executing a ProcessDefault. Also, as mentioned earlier, we don't want our build pipeline to take a long time to complete, which would be the case if Analysis Services has to process lots of data. For this reason, consider providing a reduced test dataset for unit-testing. There are various ways to do this (which I will cover later), but regardless of how you make the test data available, make sure that the data volume is small enough that a ProcessFull can execute in a timely manner (for example, less than 5 minutes).

Even if you don't want to do any unit testing or can't provide a test dataset, I still recommend executing a ProcessRecalc within the build pipeline, to make sure that the model doesn't have any bad DAX on calculated tables, calculated columns or calculation groups.

The 5 bullets above roughly correspond to the tasks that we need in our build pipeline. Once we have this pipeline set up, we can use [branch policies](https://docs.microsoft.com/en-us/azure/devops/repos/git/branch-policies?view=azure-devops#build-validation), to ensure that all changes build succesfully before a pull request can be approved.

### Task 1 - Validation deployment

Since Tabular Editor can not perform DAX syntax checking and semantic validation, we have to deploy the model to an instance of Analysis Services to make sure there are no such errors. This is exactly what we did [in the last chapter](https://tabulareditor.github.io/2019/10/08/DevOps3.html#your-first-analysis-services-build-pipeline). Let's improve the command line task a little bit, to allow changing data source connection strings and deployment credentials in a more secure manner. As mentioned in the last chapter, a better practice for passing credentials to Tabular Editor, is through environment variables. Simply add an environment variable in the Command Line task editor, pass in the corresponding pipeline variable, and modify the script to use `%EnvironmentVariableName%`:

![image](https://user-images.githubusercontent.com/8976200/66919169-1c3f2c80-f021-11e9-9fda-b74c143e649e.png)

This approach is better than what we did in the previous chapter, because we can still see the executing command in the output of the build, but the credentials will not be exposed anywhere:

![image](https://user-images.githubusercontent.com/8976200/66915235-d1211b80-f018-11e9-8f09-b2907138efdf.png)

#### Updating data sources

**Note**: The technique described below only works for Provider (legacy) data sources.

Model.bim or Database.json files normally don't store credentials for accessing the data sources used by the model. For this reason, we often have to apply these credentials after deploying. Even if we are able to use integrated security, we might still want to change the connection string of a data source, for example in order to point it to a different database. This makes sense in a multi-tiered, multi-environment BI set up: Your production models use data from your production DWH, your UAT models use data from your UAT DWH, etc.

Again, we want to make sure that we can update properties on the data source in a secure way, without exposing sensitive information in build logs, etc.

The solution is to create a Tabular Editor (C#) script, which will be executed from the command line using the `-S` switch immediately before the model is deployed. First, create the following script, and put it within a "scripts" folder in your repository - name it "SetConnectionStringFromEnv.cs":

```csharp
foreach(var dataSource in Model.DataSources.OfType<ProviderDataSource>())
{
    var evName = dataSource.Name.Replace(" ", "") + "ConnectionString";
    var evValue = Environment.GetEnvironmentVariable(evName);
    if (evValue != null)
        dataSource.ConnectionString = evValue;
}
```

This will loop through all (provider) data sources of your model, and replace the connection string on each with the value of an environment variable whose name corresponds to the name of the datasource (with spaces removed) and the "ConnectionString" suffix. So if you have a data source in your model named "SQL DW", the script will use the value of the `SQLDWConnectionString` environment variable as the connection string of the model (provided the environment variable exists).

Now, to execute the script prior to deployment, modify the command line task as follows:

```shell
TabularEditor.exe "$(Build.SourcesDirectory)\AdventureWorks" -S "$(Build.SourcesDirectory)\AdventureWorks\scripts\SetConnectionStringFromEnv.cs" -D "%ASConnectionString%" "$(ASDatabase)" -O -C -P -R -M -W -E -V
```

Also, make sure to assign whatever environment variables and pipeline variables you need:

![image](https://user-images.githubusercontent.com/8976200/66919237-409b0900-f021-11e9-9fd8-c2ce1d96be65.png)

If all goes well, Tabular Editor should output that the script was executed...

![image](https://user-images.githubusercontent.com/8976200/66918500-c9b14080-f01f-11e9-9f53-302d78c304a2.png)

...and the connection string should have been updated on the model that was deployed:

![image](https://user-images.githubusercontent.com/8976200/66918682-2c0a4100-f020-11e9-800f-cb693fb5e36b.png)

### Task 2 - Schema Check

**Note:** Schema Check only works for supported provider (legacy) data sources: OLE DB, ODBC, etc.

Now that we know how to update the connection string on our Tabular Model data sources during build, let's use the updated connection string to perform a schema check. We can use the `-SC` command line switch for this. When we use this switch, Tabular Editor will establish a connection to each data source, and compare the schema of each partition query against the imported columns of the table. If any columns are missing in the source, an error is generated. Other differences (mismatched data types or superfluous columns) will generate a warning.

While we could theoretically put the schema check into Task 1, I prefer having this as a separate step. Start by cloning the previous step, and modify the command line task as follows:

```shell
TabularEditor.exe "$(Build.SourcesDirectory)\AdventureWorks" -S "$(Build.SourcesDirectory)\AdventureWorks\scripts\SetConnectionStringFromEnv.cs" -SC -W -E -V
```

Note how this operation can be performed without deploying to Analysis Services. If you have multiple data sources, make sure to pass in each of them as environment variables:

![image](https://user-images.githubusercontent.com/8976200/66920694-1ac33380-f024-11e9-8aef-f3acf93e09bc.png)

Remember that for this to work, the agent running the build pipeline must be able to reach the data source. This means that if your source is an Azure SQL Databases, for example, the IP of the build agent needs to be whitelisted. For a Microsoft-hosted build agent, you can set the "Allow Azure services and resources to access this server": 

![image](https://user-images.githubusercontent.com/8976200/66920600-e64f7780-f023-11e9-8567-ff701d3135e9.png)

And behold - Tabular Editor now informs us that we have unmapped columns (this is only a warning, as processing should still succeed, but in the words of Brent Ozar, you're needlessly yelling data across the network):

![image](https://user-images.githubusercontent.com/8976200/66920835-61189280-f024-11e9-80e7-4b0e5f7cb6f3.png)
